<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AR and AI Integration</title>
    <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <style>
        body {
            margin: 0;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            height: 100vh;
            background-color: #f0f0f0;
        }

        model-viewer {
            width: 100%;
            height: 60%;
            max-width: 600px;
            max-height: 600px;
        }

        img {
            display: none; /* Hide the image element as it's used only for AI processing */
        }
    </style>
</head>

<body>
    <!-- AR Model Viewer -->
    <model-viewer src="https://modelviewer.dev/shared-assets/models/Astronaut.glb" ar ar-modes="webxr scene-viewer quick-look" auto-rotate camera-controls></model-viewer>

    <!-- Image element for AI processing -->
    <img id="input-image" src="envmap.jpg" alt="Input for AI">

    <script>
        // Load AI model (using a MobileNet example from TensorFlow Hub)
        async function loadModel() {
            try {
                const model = await tf.loadGraphModel('https://tfhub.dev/google/tfjs-model/imagenet/mobilenet_v2_100_224/classification/5/default/1/model.json');
                return model;
            } catch (error) {
                console.error("Failed to load AI model:", error);
            }
        }

        // Run inference on the image and get predictions
        async function runInference(model, inputElement) {
            try {
                const image = tf.browser.fromPixels(inputElement);
                const resizedImage = tf.image.resizeBilinear(image, [224, 224]);
                const batchedImage = resizedImage.expandDims(0).toFloat().div(tf.scalar(127.5)).sub(tf.scalar(1));
                const predictions = await model.predict(batchedImage).data();
                return predictions;
            } catch (error) {
                console.error("Error during inference:", error);
            }
        }

        // Handle AI predictions
        function handleAIPrediction(predictions) {
            const modelViewer = document.querySelector('model-viewer');
            const maxPrediction = Math.max(...predictions);

            // Example: Change the model's background color based on prediction confidence
            if (maxPrediction > 0.5) {
                modelViewer.style.backgroundColor = 'green';
            } else {
                modelViewer.style.backgroundColor = 'red';
            }
        }

        // Initialize AI and AR interaction
        (async () => {
            const model = await loadModel();
            const inputElement = document.getElementById('input-image');

            // Run the inference when the image loads
            inputElement.onload = async () => {
                const predictions = await runInference(model, inputElement);
                handleAIPrediction(predictions);
            };

            // Trigger the image load event manually (if the image is already cached)
            if (inputElement.complete && inputElement.naturalHeight !== 0) {
                inputElement.onload();
            }
        })();
    </script>
</body>

</html>
